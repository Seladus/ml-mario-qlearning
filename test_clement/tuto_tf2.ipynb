{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "nb_actions = len(RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "import tensorflow as tf2\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return tf2.image.rgb_to_grayscale(observation)\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = shape\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return tf2.squeeze(tf2.image.resize(observation, size=self.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(source, destination):\n",
    "    destination.set_weights(source.get_weights())\n",
    "    \n",
    "def create_model():\n",
    "    Q_star = models.Sequential()\n",
    "    Q_star.add(layers.Input((84, 84, 4)))\n",
    "    Q_star.add(layers.Conv2D(filters=32, kernel_size=8, strides=4, activation='relu'))\n",
    "    Q_star.add(layers.Conv2D(filters=64, kernel_size=4, strides=2, activation='relu'))\n",
    "    Q_star.add(layers.Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "    Q_star.add(layers.Flatten())\n",
    "    #Q_star.add(layers.Dense(512, activation='relu'))\n",
    "    Q_star.add(layers.Dense(nb_actions))\n",
    "    \n",
    "    Q_star.compile(optimizer='adam',\n",
    "        loss=Huber())\n",
    "\n",
    "    return Q_star\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\" DQN agent \"\"\"\n",
    "    def __init__(self, states, actions, max_memory, double_q):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.build_model()\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.eps = 1\n",
    "        self.eps_decay = 0.99999975 # linear decrease epsilon\n",
    "        self.eps_min = 0.1\n",
    "        self.gamma = 0.90\n",
    "        self.batch_size = 32\n",
    "        self.burnin = 500 # after this number of frames the agent starts to replace random actions by policy\n",
    "        self.copy = 10000 # copy online into target every 10000 steps\n",
    "        self.step = 0 # step counter\n",
    "        self.learn_each = 3\n",
    "        self.learn_step = 0\n",
    "        self.save_each = 500000\n",
    "        self.double_q = double_q\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\" Model builder function \"\"\"\n",
    "        self.output = create_model()\n",
    "        self.output_target = create_model()\n",
    "        self.copy_model()\n",
    "\n",
    "    def copy_model(self):\n",
    "        \"\"\" Copy weights to target network \"\"\"\n",
    "        copy_weights(self.output, self.output_target)\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\" Add observation to experience \"\"\"\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def predict(self, model, state):\n",
    "        \"\"\" Prediction \"\"\"\n",
    "        if model == 'online':\n",
    "            return self.output.predict(np.array([np.array(state, dtype=np.uint8).T]))\n",
    "        if model == 'target':\n",
    "            return self.output_target.predict(np.array(state))\n",
    "\n",
    "    def run(self, state):\n",
    "        \"\"\" Perform action \"\"\"\n",
    "        if np.random.rand() < self.eps:\n",
    "            # Random action\n",
    "            action = np.random.randint(low=0, high=self.actions)\n",
    "        else:\n",
    "            # Policy action\n",
    "            q = self.predict('online', state)\n",
    "            action = np.argmax(q)\n",
    "        # Decrease eps\n",
    "        if self.step >= self.burnin:\n",
    "            self.eps *= self.eps_decay\n",
    "            self.eps = max(self.eps_min, self.eps)\n",
    "        # Increment step\n",
    "        self.step += 1\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\" Gradient descent \"\"\"\n",
    "        # Sync target network\n",
    "        if self.step % self.copy == 0:\n",
    "            self.copy_model()\n",
    "        # Checkpoint model\n",
    "        # TODO : save model\n",
    "        # Break if burn-in\n",
    "        if self.step < self.burnin:\n",
    "            return\n",
    "        # Break if no training\n",
    "        if self.learn_step < self.learn_each:\n",
    "            self.learn_step += 1\n",
    "            return\n",
    "        # Sample batch from memory\n",
    "        batch = random.sample(agent.memory, agent.batch_size)\n",
    "        state, next_state, action, reward, done = map(np.array, zip(*batch))\n",
    "        features = np.array([A.T for A in next_state])\n",
    "        # Compute estimated reward for each state un the batch (building targets to learn with)\n",
    "        predictions = agent.output_target.predict(features)\n",
    "        targets = reward + (1 - done) * agent.gamma * np.amax(predictions, axis=1)\n",
    "        agent.output.fit(x=features, y=targets, batch_size=agent.batch_size, epochs=1, verbose=0)\n",
    "        # Reset learn step\n",
    "        self.learn_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "Episode 0 - +Frame 36 - +Frames/sec 88.0 - +Epsilon 1 - +Mean Reward 6.527777777777778\n",
      "True\n",
      "Episode 1 - +Frame 191 - +Frames/sec 101.0 - +Epsilon 1 - +Mean Reward 5.3187275985663085\n",
      "True\n",
      "Episode 2 - +Frame 342 - +Frames/sec 103.0 - +Epsilon 1 - +Mean Reward 5.85045416063361\n",
      "True\n",
      "Episode 3 - +Frame 428 - +Frames/sec 106.0 - +Epsilon 1 - +Mean Reward 6.178538294893812\n",
      "True\n",
      "Episode 4 - +Frame 583 - +Frames/sec 51.0 - +Epsilon 1.0 - +Mean Reward 5.99057257139892\n",
      "True\n",
      "Episode 5 - +Frame 960 - +Frames/sec 39.0 - +Epsilon 0.9999 - +Mean Reward 5.331666355918199\n",
      "True\n",
      "Episode 6 - +Frame 1704 - +Frames/sec 41.0 - +Epsilon 0.9997 - +Mean Reward 4.747035063905307\n",
      "True\n",
      "Episode 7 - +Frame 2485 - +Frames/sec 38.0 - +Epsilon 0.9995 - +Mean Reward 4.263610866576554\n",
      "True\n",
      "Episode 8 - +Frame 2675 - +Frames/sec 40.0 - +Epsilon 0.9995 - +Mean Reward 4.226718431108984\n",
      "True\n",
      "Episode 9 - +Frame 2957 - +Frames/sec 40.0 - +Epsilon 0.9994 - +Mean Reward 4.070713254664752\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "\n",
    "# Build env (first level, right only)\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=(84,84))\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "# Parameters\n",
    "states = (84, 84, 4)\n",
    "actions = nb_actions\n",
    "\n",
    "# Agent\n",
    "agent = DQNAgent(states=states, actions=actions, max_memory=10000, double_q=True)\n",
    "\n",
    "# Episodes\n",
    "episodes = 10\n",
    "rewards = []\n",
    "\n",
    "# Timing\n",
    "start = time.time()\n",
    "step = 0\n",
    "\n",
    "# Main loop\n",
    "for e in range(episodes):\n",
    "\n",
    "    # Reset env\n",
    "    state = env.reset()\n",
    "\n",
    "    # Reward\n",
    "    total_reward = 0\n",
    "    iter = 0\n",
    "\n",
    "    # Play\n",
    "    while True:\n",
    "\n",
    "        # Show env (disabled)\n",
    "        env.render()\n",
    "\n",
    "        # Run agent\n",
    "        action = agent.run(state=state)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, info = env.step(action=action)\n",
    "\n",
    "        # Remember transition\n",
    "        agent.add(experience=(state, next_state, action, reward, done))\n",
    "\n",
    "        # Update agent\n",
    "        agent.learn()\n",
    "\n",
    "        # Total reward\n",
    "        total_reward += reward\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Increment\n",
    "        iter += 1\n",
    "\n",
    "        # If done break loop\n",
    "        if done or info['flag_get']:\n",
    "            print(done)\n",
    "            break\n",
    "\n",
    "    # Rewards\n",
    "    rewards.append(total_reward / iter)\n",
    "\n",
    "    # Print\n",
    "    if e % 1 == 0:\n",
    "        print('Episode {e} - +'\n",
    "              'Frame {f} - +'\n",
    "              'Frames/sec {fs} - +'\n",
    "              'Epsilon {eps} - +'\n",
    "              'Mean Reward {r}'.format(e=e,\n",
    "                                       f=agent.step,\n",
    "                                       fs=np.round((agent.step - step) / (time.time() - start)),\n",
    "                                       eps=np.round(agent.eps, 4),\n",
    "                                       r=np.mean(rewards[-100:])))\n",
    "        start = time.time()\n",
    "        step = agent.step\n",
    "\n",
    "env.close()"
   ]
  }
 ]
}